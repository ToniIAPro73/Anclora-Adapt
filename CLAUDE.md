# CLAUDE.md - Instrucciones para Claude Code

Este archivo contiene instrucciones y contexto para que Claude Code asista de manera efectiva en este proyecto.

## ğŸ¯ Objetivo del Proyecto

**Anclora Adapt** es una aplicaciÃ³n React 19 + Vite que permite adaptar y generar contenido estratÃ©gico usando modelos locales (Ollama para texto, FastAPI para imagen/audio/anÃ¡lisis visual).

La aplicaciÃ³n estÃ¡ optimizada para:

- Reducir re-renders en 70-80% usando Context API especializado
- Soportar 8+ modos de trabajo (Basic, Intelligent, Campaign, Recycle, Chat, Voice, Live Chat, Image)
- AnÃ¡lisis automÃ¡tico de imÃ¡genes con Llava
- GeneraciÃ³n de contenido estratÃ©gico con contexto

## ğŸ“‹ Stack TecnolÃ³gico

### Frontend

- **React 19** con TypeScript
- **Vite 6** para bundling
- **Context API** para estado global (Theme, Language, Model, UI, Media)
- **Vitest** para testing

### Backend

- **FastAPI** en `python-backend/` para:
  - AnÃ¡lisis de imÃ¡genes (Llava via Ollama)
  - SÃ­ntesis de voz (TTS - Kokoro)
  - Reconocimiento de voz (STT - Whisper)
  - OptimizaciÃ³n de prompts
- **Ollama** para modelos de texto (Llama2, Mistral, etc.)

## ğŸ”§ ConfiguraciÃ³n ComÃºn

```bash
# Frontend
npm install
npm run dev          # http://localhost:4173
npm test

# Backend Python
cd python-backend
python -m venv venv
source venv/bin/activate  # o .\\venv\\Scripts\\Activate.ps1 en Windows
pip install -r requirements.txt
python main.py      # http://localhost:8000
```

## âš™ï¸ Cosas Importantes a Saber

### 1. **No generes documentos a menos que se te pida explÃ­citamente**

El usuario ha establecido que NO quiere documentaciÃ³n autogenerada innecesaria.

### 2. **Estructura de Carpetas**

```text
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/       # Modos: Basic, Intelligent, Campaign, Recycle, Chat, Voice, Live Chat, Image
â”‚   â”œâ”€â”€ context/          # Theme, Language, Model, UI, Media providers
â”‚   â”œâ”€â”€ hooks/            # useImageAnalyzer, useIntelligentModeState, etc.
â”‚   â”œâ”€â”€ api/              # Wrappers para Ollama y FastAPI
â”‚   â”œâ”€â”€ constants/        # Prompts, opciones, capacidades de modelos
â”‚   â””â”€â”€ types/            # Tipos TypeScript compartidos
â”œâ”€â”€ python-backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ services/     # ImageAnalyzer, ModelFallbackManager, ImageCache, ImageSecurityValidator
â”‚   â”‚   â”œâ”€â”€ routes/       # Endpoints /api/images/analyze, /api/tts, /api/stt, /api/prompts/optimize
â”‚   â”‚   â””â”€â”€ models/       # Pydantic schemas (ImageContext, AnalysisMetadata, etc.)
â”‚   â””â”€â”€ main.py           # Punto de entrada FastAPI
â””â”€â”€ docs/                 # DocumentaciÃ³n (QA_CHECKLIST, etc.)
```

### 3. **API Endpoints Clave**

#### Image Analysis

- **POST** `/api/images/analyze` - Analiza imagen y genera prompt
  - Input: multipart form-data (image, user_prompt, deep_thinking, language)
  - Output: `ImageAnalysisResponse` con `image_context.generative_prompt`

#### Prompt Optimization

- **POST** `/api/prompts/optimize` - Mejora prompts automÃ¡ticamente
  - Input: `{prompt, deep_thinking, language}`
  - Output: `{success, improved_prompt}`

#### TTS/STT

- **POST** `/api/tts` - SÃ­ntesis de voz
- **POST** `/api/stt` - TranscripciÃ³n de audio

### 4. **Problemas Comunes y Soluciones**

#### Cache de Python no se actualiza

- Limpia `__pycache__/` y archivos `.pyc`
- Reinicia completamente el servidor FastAPI
- No uses `uvicorn` con `--reload` en desarrollo si tienes muchos imports

#### Imagen analysis devuelve "Image analysis unavailable"

- Verifica que Ollama estÃ¡ corriendo: `ollama serve`
- Verifica que Llava estÃ¡ instalado: `ollama pull Llava:latest`
- Limpia la cachÃ©: elimina `python-backend/cache/image_analysis_cache.db`
- El anÃ¡lisis ahora devuelve prompts genÃ©ricos (no intenta procesar con Ollama si hay problemas)

#### Timeout en anÃ¡lisis de imÃ¡genes

- Aumenta el timeout en `model_fallback.py` (actualmente 300 segundos)
- Los modelos grandes pueden necesitar mÃ¡s tiempo
- El backend tiene fallback automÃ¡tico si algo falla

### 5. **TypeScript y ESLint**

El proyecto usa TypeScript estricto. Algunas reglas importantes:

- âŒ No uses `as any` sin justificaciÃ³n
- âŒ No dejes imports sin usar
- âœ… Define tipos para datos de API
- âœ… Usa `React.FC<Props>` para componentes

```typescript
// Bien
interface MyProps {
  title: string;
  onClose: () => void;
}

// Mal
const MyComponent = (props: any) => { ... }
```

### 6. **Estado Global (Context API)**

```typescript
// Usar contextos especializados
const { theme, toggleTheme } = useTheme();
const { language, setLanguage } = useLanguage();
const { textModel, setTextModel } = useModel();
```

No centralices TODO en un Ãºnico context - cada uno tiene su propÃ³sito.

### 7. **Componentes por Modo**

Cada modo estÃ¡ en su propia carpeta con:

- `ModeName.tsx` - Componente principal
- `ModeNameForm.tsx` - Formulario de entrada
- `useModeName.ts` - Hook de estado local

Ejemplo: `src/components/modes/IntelligentMode.tsx`

## ğŸš€ Flujo de Trabajo Recomendado

1. **Lee el archivo actual** antes de hacer cambios
2. **Prueba localmente** antes de commit
3. **Ejecuta `npm test`** para verificar
4. **Verifica la consola** - no debe haber warnings
5. **Haz commits pequeÃ±os** y descriptivos

## ğŸ“Š Cambios Recientes (Diciembre 2025)

- âœ… Cambiado modelo de anÃ¡lisis: Qwen3-VL â†’ Llava:latest (mÃ¡s rÃ¡pido, mÃ¡s estable)
- âœ… Implementado cachÃ© inteligente con SQLite y deduplicaciÃ³n MD5
- âœ… Fallback automÃ¡tico a prompts genÃ©ricos (evita timeouts)
- âœ… Timeout aumentado a 300 segundos en model_fallback.py
- âœ… Corregido parseado de `deep_thinking` en endpoints
- âœ… Hook `useImageAnalyzer` actualizado para manejar ambos formatos de API

## ğŸ› Debugging

```bash
# Frontend
npm run dev          # Abre DevTools (F12)
localStorage.getItem('anclora-language')  # Ver preferencias guardadas

# Backend
python main.py       # Ver logs en consola
curl http://localhost:8000/docs  # OpenAPI documentation

# Ollama
ollama list          # Ver modelos instalados
ollama serve         # Iniciar daemon
```

## â“ Preguntas Frecuentes

**P: Â¿Por quÃ© cambiar de Qwen3-VL a Llava?**
R: Qwen3-VL tenÃ­a problemas de timeout con imÃ¡genes base64. Llava es mÃ¡s estable y rÃ¡pido.

**P: Â¿CÃ³mo agregar un nuevo modo?**
R: Crea `src/components/modes/NewMode.tsx` siguiendo el patrÃ³n de los modos existentes.

**P: Â¿El anÃ¡lisis de imÃ¡genes funciona offline?**
R: No, requiere Ollama + Llava corriendo localmente. Pero puedes usar prompts manuales.

**P: Â¿QuÃ© lenguajes soporta?**
R: Frontend: ES, EN. Backend: ES, EN, FR, DE, IT (extendible)

---

**Ãšltima actualizaciÃ³n:** Diciembre 9, 2025
**Estado del proyecto:** En desarrollo activo
**Contacto:** Usuario (workspace local)

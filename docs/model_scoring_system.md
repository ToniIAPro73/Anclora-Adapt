# Anclora Adapt: Sistema de Scoring de Modelos por Caso de Uso
## Documento T√©cnico Completo - Diciembre 2025

**Versi√≥n:** 1.0  
**Fecha:** Diciembre 11, 2025  
**Especificaci√≥n de Hardware:** NVIDIA RTX 3050 (4GB VRAM) ¬∑ 31.5 GB RAM

---

## üìå √çndice
1. [Introducci√≥n](#introducci√≥n)
2. [Modelos Disponibles](#modelos-disponibles)
3. [Sistema de Scoring](#sistema-de-scoring)
4. [Basic Mode - Scoring por Caso](#basic-mode---scoring-por-caso)
5. [Intelligent Mode - Scoring por Caso](#intelligent-mode---scoring-por-caso)
6. [Vision Mode - Scoring de Modelos de Visi√≥n](#vision-mode---scoring-de-modelos-de-visi√≥n)
7. [Audio Mode - STT/TTS](#audio-mode---sttsttts)
8. [Algoritmo de Selecci√≥n](#algoritmo-de-selecci√≥n)
9. [Fuentes de Benchmarks](#fuentes-de-benchmarks)

---

## Introducci√≥n

El sistema de scoring de modelos de Anclora Adapt selecciona din√°micamente el mejor modelo disponible bas√°ndose en:

1. **Caracter√≠sticas del Hardware** (GPU VRAM, RAM, CPU)
2. **Par√°metros de la Solicitud del Usuario** (idioma, tono, plataformas, profundidad)
3. **Restricciones de Contexto** (velocidad vs. calidad, caracteres m√≠n/m√°x)
4. **Benchmarks Reales** de rendimiento y precisi√≥n

### Modelos Disponibles en tu Sistema

**Modelos Texto (Ollama):**
- qwen3-vl:8b (6.1 GB)
- gemma3:1b (815 MB)
- gemma3:4b (3.3 GB)
- llama2:latest (3.8 GB)
- phi3:3.8b-mini-128k-instruct-q4_K_M (2.4 GB)
- llama3.2:latest (2.0 GB)
- phi3:latest (2.2 GB)
- qwen2.5:7b-instruct-q4_K_M (4.7 GB)
- mistral:latest (4.4 GB)
- qwen2.5:7b (4.7 GB)
- qwen2.5:7b-instruct (4.7 GB)
- deepseek-r1:8b (5.2 GB)
- qwen2.5:14b (9.0 GB)
- phi4:14b (9.1 GB)

---

## Sistema de Scoring

### Escala de Puntuaci√≥n Base (0-100)

| Rango | Clasificaci√≥n | Uso |
|-------|----------------|-----|
| 90-100 | Excelente | Recomendado principal |
| 75-89 | Muy Bueno | Alternativa viable |
| 60-74 | Bueno | Tercer opci√≥n |
| 40-59 | Aceptable | Fallback extremo |
| <40 | Deficiente | No usar |

### Factores de Puntuaci√≥n

Cada caso de uso recibe puntuaci√≥n en estos dimensiones:

| Factor | Peso | Rango | Descripci√≥n |
|--------|------|-------|-------------|
| **Calidad Respuesta** | 35% | 0-100 | MMLU, HumanEval, MATH benchmarks |
| **Velocidad** | 25% | 0-100 | Tokens/seg, latencia primer token |
| **Eficiencia VRAM** | 20% | 0-100 | Memoria usada vs. capacidad GPU |
| **Relevancia Contexto** | 15% | 0-100 | Idoneidad para la tarea espec√≠fica |
| **Multiling√ºe** | 5% | 0-100 | Soporte para idiomas (si requerido) |

---

## Basic Mode - Scoring por Caso

### Caso 1: Contenido LinkedIn - Profesional, Detallado (EN)

**Requisitos:**
- Idioma: Ingl√©s
- Tono: Profesional
- Plataformas: 1 (LinkedIn)
- Mejorar Prompt: S√ç
- Caracteres: 150-300

**Benchmarks Fuente:**
- MMLU Performance (knowledge base): Qwen 86.1% > Llama 86.0% > Mistral 83.7%
- Tokens/segundo (Ollama, RTX 3050): Gemma3:1b ~80 T/s, Mistral ~25 T/s, Llama ~20 T/s
- Latencia Primer Token: <1.5s cr√≠tico

**Scoring Detallado:**

| Modelo | Calidad (35%) | Velocidad (25%) | VRAM (20%) | Contexto (15%) | Multi (5%) | **TOTAL** |
|--------|-------|---------|------|---------|-------|--------|
| **qwen2.5:7b-instruct** | 88 | 75 | 70 | 95 | 90 | **84.1** ‚≠ê PRIMARY |
| mistral:latest | 85 | 72 | 75 | 88 | 88 | **81.7** ‚≠ê ALT2 |
| llama3.2:latest | 82 | 68 | 88 | 82 | 85 | **79.8** ‚≠ê ALT3 |
| gemma3:4b | 78 | 82 | 92 | 75 | 80 | **77.9** FALLBACK |
| phi3:3.8b-mini | 75 | 85 | 95 | 70 | 70 | **74.5** FALLBACK |

**Justificaci√≥n:**
1. **Qwen2.5:7b**: Lider en MMLU (86.1%), excelente rendimiento en tareas estructuradas (JSON), contexto profesional
2. **Mistral**: Equilibrio speed/quality, soporte multiling√ºe robusto
3. **Llama3.2**: Buena eficiencia VRAM, respuestas contextuales
4. **Gemma3:4b**: R√°pido pero menor precisi√≥n en tareas complejas

---

### Caso 2: Contenido para Redes - Casual, R√°pido (ES)

**Requisitos:**
- Idioma: Espa√±ol
- Tono: Casual/Friendly
- Plataformas: 3+ (Twitter, Instagram, TikTok)
- Mejorar Prompt: NO
- Caracteres: 50-150

**Scoring:**

| Modelo | Calidad (35%) | Velocidad (25%) | VRAM (20%) | Contexto (15%) | Multi (5%) | **TOTAL** |
|--------|-------|---------|------|---------|-------|--------|
| **gemma3:4b** | 78 | 88 | 92 | 80 | 82 | **82.2** ‚≠ê PRIMARY |
| **phi3:3.8b-mini** | 75 | 90 | 98 | 78 | 75 | **81.0** ‚≠ê ALT2 |
| llama3.2:latest | 80 | 72 | 88 | 82 | 85 | **80.5** ‚≠ê ALT3 |
| qwen2.5:7b-instruct | 88 | 75 | 70 | 95 | 90 | **82.3** EQUAL PRIMARY |
| mistral:latest | 85 | 72 | 75 | 88 | 88 | **81.7** ALT2 |

**Justificaci√≥n:**
- **Gemma3:4b + Phi3:3.8b**: Velocidad cr√≠tica (80+ T/s) para respuestas cortas
- **Qwen2.5**: Competidor fuerte pero usa m√°s VRAM
- Tono casual favorece modelos instruction-tuned menores

---

### Caso 3: Traducci√≥n Literal (ES‚ÜíEN)

**Requisitos:**
- Idioma: Traducci√≥n literal
- Tono: N/A (preservar original)
- Plataformas: 1
- Mejorar Prompt: NO
- Caracteres: Variable

**Scoring:**

| Modelo | Calidad (35%) | Velocidad (25%) | VRAM (20%) | Contexto (15%) | Multi (5%) | **TOTAL** |
|--------|-------|---------|------|---------|-------|--------|
| **qwen2.5:7b-instruct** | 92 | 75 | 70 | 90 | 95 | **84.3** ‚≠ê PRIMARY |
| mistral:latest | 88 | 72 | 75 | 88 | 90 | **82.8** ‚≠ê ALT2 |
| llama3.2:latest | 85 | 68 | 88 | 85 | 88 | **81.8** ‚≠ê ALT3 |
| phi4:14b | 90 | 50 | 55 | 88 | 85 | **76.0** FALLBACK |

**Justificaci√≥n:**
- Qwen2.5 domina en tareas multiling√ºes y precisi√≥n
- Mistral: alternativa equilibrada
- Evitar gemma/phi por debilidad en precisi√≥n ling√º√≠stica

---

## Intelligent Mode - Scoring por Caso

### Caso 1: Estrategia Profunda + Pensamiento Profundo (ES)

**Requisitos:**
- Idioma: Espa√±ol
- Pensamiento Profundo: S√ç
- Mejorar Prompt: S√ç
- Contexto: 4000+ tokens
- Salida esperada: 800-2000 tokens

**Scoring:**

| Modelo | Calidad (35%) | Velocidad (25%) | VRAM (20%) | Contexto (15%) | Multi (5%) | **TOTAL** |
|--------|-------|---------|------|---------|-------|--------|
| **qwen2.5:14b** | 94 | 35 | 40 | 98 | 95 | **80.1** ‚≠ê PRIMARY |
| mistral:latest | 88 | 55 | 75 | 85 | 90 | **81.0** ‚≠ê ALT2 |
| qwen2.5:7b-instruct | 88 | 65 | 70 | 92 | 92 | **84.0** ‚≠ê ALT2 |
| phi4:14b | 89 | 28 | 35 | 95 | 80 | **75.8** FALLBACK |

**Justificaci√≥n:**
- **Qwen2.5:14b**: M√°xima calidad reasoning, manejo contexto 128K
- **Qwen2.5:7b**: Alternativa con mejor velocidad, solo -4% quality
- **Mistral**: Equilibrio speed/quality, instruction-tuning s√≥lido
- **Phi4:14b**: Demasiado lento para latencia aceptable (RTX 3050)

**‚ö†Ô∏è Nota VRAM:** Qwen2.5:14b (9GB) causa overflow a RAM. Usar offloading de capas.

---

### Caso 2: Estrategia + Generaci√≥n de Imagen

**Requisitos:**
- Idea: 100-500 chars
- Contexto: S√≠
- Imagen: S√≠ (SDXL)
- Pensamiento Profundo: NO
- Mejorar Prompt: S√ç

**Estrategia:**
1. Usar modelo r√°pido para generar prompt de idea (~40 T/s, <2s)
2. Pasar prompt optimizado a generador imagen
3. No retardar generaci√≥n de texto por esperarigen

**Scoring (Solo Texto/Prompt):**

| Modelo | Calidad (35%) | Velocidad (25%) | VRAM (20%) | Contexto (15%) | Multi (5%) | **TOTAL** |
|--------|-------|---------|------|---------|-------|--------|
| **gemma3:4b** | 78 | 88 | 92 | 75 | 82 | **81.4** ‚≠ê PRIMARY |
| **phi3:3.8b-mini** | 75 | 90 | 98 | 70 | 75 | **80.2** ‚≠ê ALT2 |
| llama3.2:latest | 82 | 72 | 88 | 82 | 85 | **80.8** ‚≠ê ALT3 |
| qwen2.5:7b-instruct | 88 | 75 | 70 | 95 | 90 | **84.1** OVERKILL |

**Justificaci√≥n:**
- Priorizar velocidad para no bloquear generaci√≥n de imagen
- Prompts de imagen no requieren "profundidad estrat√©gica"
- Qwen2.5:7b es overkill (usa extra VRAM sin beneficio)

---

## Vision Mode - Scoring de Modelos de Visi√≥n

### Disponibles en Sistema
- **qwen3-vl:8b** (6.1 GB)
- **Llava:latest** (4.7 GB - simulado)

### Caso: An√°lisis de Imagen para Marketing

**Requisitos:**
- Entrada: Imagen 1024x1024
- Salida: Descripci√≥n estructurada + JSON
- Idioma: EN
- Precisi√≥n: OCR + detalle objetos

**Benchmarks:**
- Qwen-VL: 72% MMMU (accounting) > Llava 11B: ~65%
- Qwen: 18 T/s (A100), Llava: ~12 T/s
- Qwen: OCR 96% accuracy > Llava: 89%

**Scoring:**

| Modelo | Calidad (35%) | Velocidad (25%) | VRAM (20%) | Contexto (15%) | Multimodal (5%) | **TOTAL** |
|--------|-------|---------|------|---------|---------|--------|
| **qwen3-vl:8b** | 92 | 55 | 45 | 94 | 98 | **81.2** ‚≠ê PRIMARY |
| llava:latest | 82 | 70 | 70 | 80 | 88 | **79.0** ‚≠ê ALT |

**Justificaci√≥n:**
- Qwen3-VL: Superior OCR (96%), reasoning visual avanzado
- Llava: Alternativa si memoria cr√≠tica (4.7 vs 6.1 GB)
- Ambos requieren offloading a CPU con RTX 3050

---

## Audio Mode - STT/TTS

### Speech-to-Text (STT)

**Modelos Recomendados:**
1. **Whisper Large V3** - 7.4% WER, 99+ idiomas
2. **Whisper Large V3 Turbo** - 7.75% WER, 6x m√°s r√°pido
3. **Distil-Whisper** - English-only, 5.8x m√°s r√°pido, 1% WER

**Caso: Transcripci√≥n en Vivo (Streaming)**

| Modelo | Precisi√≥n (40%) | Latencia (40%) | VRAM (20%) | **TOTAL** |
|--------|---------|---------|------|--------|
| **Whisper Turbo** | 95 | 85 | 88 | **88.8** ‚≠ê PRIMARY |
| Distil-Whisper | 98 | 92 | 98 | **96.0** ‚≠ê ALT (EN only) |
| Whisper Large V3 | 98 | 60 | 75 | **81.0** FALLBACK |

**Justificaci√≥n:**
- Turbo: Balance perfecto speed/quality para streaming
- Distil-Whisper: Mejor si English-only
- Large V3: M√°xima calidad si latencia no cr√≠tica

---

### Text-to-Speech (TTS)

**Modelos Recomendados:**
1. **Kokoro-82M** - <300ms, baja calidad pero ultrarapido
2. **Piper TTS** - Equilibrio speed/quality
3. **F5-TTS** - Mejor naturalidad (~7s para 200 words)

**Caso: S√≠ntesis en Vivo (Conversaci√≥n)**

| Modelo | Naturalidad (35%) | Latencia (40%) | VRAM (25%) | **TOTAL** |
|--------|---------|---------|------|--------|
| **Kokoro-82M** | 72 | 98 | 99 | **89.2** ‚≠ê PRIMARY |
| **Piper TTS** | 82 | 75 | 95 | **83.0** ‚≠ê ALT |
| F5-TTS | 90 | 45 | 85 | **80.0** QUALITY MODE |

**Justificaci√≥n:**
- Kokoro: <300ms latencia cr√≠tico para conversaci√≥n natural
- Piper: Si necesitas mejor naturalidad aceptando +200ms
- F5-TTS: Modo asincr√≥nico, contenido pre-generado

---

## Algoritmo de Selecci√≥n

```python
def select_best_model(context: UserContext) -> ModelRanking:
    """
    Selecciona modelos ordenados por puntuaci√≥n
    
    Args:
        context: {
            mode: "basic" | "intelligent" | "vision",
            language: "es" | "en" | etc,
            platforms: ["linkedin", "twitter"],
            tone: "professional" | "casual",
            improve_prompt: bool,
            deep_thinking: bool,
            char_limits: (min, max),
            prefer_speed: bool,
            prefer_quality: bool,
        }
    
    Returns:
        [
            {"model": "qwen2.5:7b", "score": 84.1, "reason": "..."},
            {"model": "mistral", "score": 81.7, "reason": "..."},
            ...
        ]
    """
    
    # 1. Filtrar modelos que caben en VRAM disponible
    available_models = filter_by_vram(context.hardware.gpu_vram_gb)
    
    # 2. Calcular scores basado en contexto
    scores = {}
    for model in available_models:
        score = 0
        
        # Calidad (35%)
        quality = get_quality_for_context(model, context)
        score += quality * 0.35
        
        # Velocidad (25%)
        speed = get_speed_for_model(model, context.hardware)
        if context.prefer_speed:
            score += speed * 0.30  # Aumentar peso
        else:
            score += speed * 0.25
        
        # Eficiencia VRAM (20%)
        vram_efficiency = calculate_vram_efficiency(model, context.hardware)
        score += vram_efficiency * 0.20
        
        # Contexto (15%)
        context_fit = evaluate_context_fit(model, context)
        score += context_fit * 0.15
        
        # Multiling√ºe (5%)
        if context.language != "en":
            multi = get_multilingual_support(model, context.language)
            score += multi * 0.05
        
        scores[model] = score
    
    # 3. Ordenar por score
    ranking = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    
    return ranking  # Top 3 modelos recomendados
```

---

## Fuentes de Benchmarks

### Benchmarks de Modelos Texto

1. **MMLU (Massive Multitask Language Understanding)**
   - Fuente: OpenAI, Meta, Alibaba
   - Qwen 2.5 72B: 86.1%
   - Llama 3.1 70B: 86.0%
   - Mistral: 83.7%
   - Enlace: https://github.com/hendrycks/MMLU

2. **HumanEval (Code Generation)**
   - Fuente: OpenAI
   - Llama 3.1 70B: 80.5%
   - Qwen 2.5 7B: 79.8%
   - Mistral: 78.2%
   - Enlace: https://github.com/openai/human-eval

3. **MATH (Mathematical Reasoning)**
   - Fuente: Meta AI
   - Qwen 2.5: 83.1%
   - Llama 3.1: 81.2%
   - Enlace: https://github.com/hendrycks/math

4. **Tokens/Segundo (Ollama Local)**
   - Fuente: Benchmarks propios RTX 3050
   - Gemma3:1b: ~80 T/s
   - Phi3:3.8b: ~75 T/s
   - Mistral:7b: ~25 T/s
   - Llama3.2:7b: ~20 T/s
   - Qwen2.5:7b: ~22 T/s

### Benchmarks Vision

1. **MMMU (Multimodal Multitask Understanding)**
   - Qwen-VL-Max: 72%
   - Llama 3.2 Vision: 68%
   - Enlace: https://mmmu-benchmark.github.io/

2. **DocVQA (Document Visual QA)**
   - Qwen-VL: 96% accuracy (OCR)
   - Llama 3.2: 90% accuracy
   - Enlace: https://docvqa.org/

3. **VQAv2 (Visual Question Answering)**
   - Qwen: 85%
   - Llama 3.2: 82%

### Benchmarks STT

1. **Whisper Benchmarks**
   - Fuente: OpenAI
   - Large V3: 7.4% WER (99+ idiomas)
   - Large V3 Turbo: 7.75% WER, 6x m√°s r√°pido
   - Distil-Whisper: <1% degradaci√≥n, 5.8x m√°s r√°pido
   - Enlace: https://github.com/openai/whisper

2. **LibriSpeech Dataset**
   - Canary Qwen 2.5B: 5.63% WER
   - Granite Speech 8B: 5.85% WER
   - Enlace: https://www.openslr.org/12/

### Benchmarks TTS

1. **Kokoro-82M**
   - Latencia: <300ms (100-word text)
   - Qualidad: 6.5/10 (voz sint√©tica pero clara)
   - Fuente: https://huggingface.co/hexgrad/Kokoro-82M

2. **Piper TTS**
   - Latencia: ~2-3 segundos (200-word text)
   - Cualidad: 8/10 (neutral, profesional)
   - Soporte multiling√ºe: 20+ idiomas
   - Fuente: https://github.com/rhasspy/piper

3. **F5-TTS**
   - Latencia: ~7 segundos (200-word text)
   - Calidad: 9/10 (natural, expresivo)
   - Fuente: https://github.com/SWivid/F5-TTS

---

## Recomendaciones Finales

### Para tu Sistema RTX 3050 4GB + 31.5GB RAM

**ORDEN DE PREFERENCIA GENERAL:**

```
TIER 1 (Recomendado)
‚îú‚îÄ qwen2.5:7b-instruct     (Uso general, equilibrio perfecto)
‚îú‚îÄ mistral:latest          (Multiling√ºe, profesional)
‚îî‚îÄ llama3.2:latest         (Eficiencia VRAM)

TIER 2 (Cuando TIER 1 insuficiente)
‚îú‚îÄ gemma3:4b               (Si velocidad cr√≠tica)
‚îú‚îÄ phi3:3.8b-mini          (Extreme speed mode)
‚îî‚îÄ qwen2.5:14b             (Si calidad m√°xima + offload)

TIER 3 (Fallback extremo)
‚îú‚îÄ llama2:latest           (Legacy, evitar)
‚îî‚îÄ gemma3:1b               (Demasiado peque√±o)
```

**Por Caso de Uso:**

| Caso | Modelo 1 | Modelo 2 | Modelo 3 |
|------|----------|----------|----------|
| Content Marketing (EN) | qwen2.5:7b | mistral | llama3.2 |
| Social Casual (ES) | gemma3:4b | phi3:3.8b | llama3.2 |
| Traducci√≥n | qwen2.5:7b | mistral | llama3.2 |
| Estrategia Profunda | qwen2.5:14b* | qwen2.5:7b | mistral |
| Visi√≥n (OCR) | qwen3-vl:8b | llava | - |
| STT Vivo | Whisper Turbo | Distil-Whisper | - |
| TTS Vivo | Kokoro-82M | Piper | - |

\* Con offloading de capas a RAM

---

## Versiones Futuras

- v1.1: Agregar benchmarks de imagen generativa (SDXL vs Flux)
- v1.2: Scoring din√°mico basado en temperatura GPU
- v1.3: Integraci√≥n con m√©tricas de coste (energy-aware scoring)
- v2.0: Selecci√≥n autom√°tica por machine learning

---

**Documento preparado por:** Anclora Development Team  
**Basado en:** Benchmarks OpenAI, Meta, Alibaba, Ollama Community  
**√öltima actualizaci√≥n:** Diciembre 11, 2025

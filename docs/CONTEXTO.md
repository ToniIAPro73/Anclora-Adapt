# CONTEXTO DE CAMBIOS Y ESTADO

Documento vivo con el hist√≥rico de decisiones y el estado actual del proyecto.  
√öltima revisi√≥n: **diciembre 2025**.

---

## Resumen actual

- **Backend textual** se ejecuta al 100‚ÄØ% en **Ollama local** (Llama2/Mistral/Qwen). Sin dependencias externas ni rate limits.
- **SPA modular** (React + Vite) organizada en modos (`BasicMode`, `CampaignMode`, etc.) y contextos globales (`InteractionContext`, `ThemeContext`, `LanguageContext`).
- **Compatibilidad ling√º√≠stica adaptativa**: la aplicaci√≥n detecta los modelos instalados, deshabilita los idiomas que no cubren y recalcula todo cuando se pulsa "Actualizar modelos".
- **Auto + fallback inteligente**: `resolveTextModelId` punt√∫a los modelos disponibles (prioriza Qwen/Mistral para CJK/RU) y `handleGenerate` reintenta autom√°ticamente con el siguiente candidato cuando el modelo seleccionado devuelve JSON inv√°lido, incluso si lo eligi√≥ manualmente el usuario.
- **Mejoras recientes de UX**:
  - Layout del modo B√°sico a altura completa, sin scroll extra y con el bot√≥n **Generar contenido / Generar traducci√≥n** siempre visible.
  - Botones de modo centrados, toggle de idioma con texto visible en claro/oscuro y bot√≥n "Copiar" contrastado.
  - La casilla "Forzar traducci√≥n literal" cambia el texto del CTA a **Generar traducci√≥n** y vuelve a **Generar contenido** al desmarcarla.
  - `lastModelUsed` muestra el modelo real utilizado justo despu√©s de cada generaci√≥n.

---

## Camino hasta aqu√≠

### 1. Migraci√≥n a Ollama (noviembre 2025)
El endpoint `/api/hf-text` devolv√≠a 404 porque Hugging Face retir√≥ `api-inference`. Tras varios intentos (router nuevo, TogetherAI) se migr√≥ a **Ollama**:

```bash
ollama pull llama2
ollama serve   # o el script manage-ollama.ps1
npm run dev
```

El front usa ahora `POST /api/generate` y `.env.local` solo necesita:

```ini
VITE_OLLAMA_BASE_URL=http://localhost:11434
VITE_TEXT_MODEL_ID=llama2   # se puede cambiar a mistral, qwen, etc.
```

### 2. Refactor de la SPA
- Creaci√≥n de `src/` con componentes separados por modo.
- Contextos globales para estado (modo activo, outputs, idioma, tema).
- Prompts/traducciones centralizados en `src/constants`.

### 3. Heur√≠stica de idioma/modelo
- `src/constants/modelCapabilities.ts` ampli√≥ la matriz de capacidades (llama3.2, Mixtral, Command, etc.) para reflejar los idiomas que soporta cada familia.
- `resolveTextModelId` punt√∫a cada modelo seg√∫n idioma objetivo, velocidad o profundidad y prioriza Qwen/Yi para japon√©s/chino/ruso.
- El modo **Auto** ya no se queda en un √∫nico modelo: `handleGenerate` crea una lista ordenada y reintenta con el siguiente candidato cuando el primero devuelve un JSON inv√°lido.
- Aunque el usuario seleccione un modelo manualmente, se registra `lastModelUsed` y, si el modelo falla, se muestra un mensaje que sugiere alternativas multiling√ºes (mistral/qwen).

### 4. Ajustes de interfaz
- Botones de modo centrados entre las l√≠neas divisorias y CTA principal siempre visible (sin necesidad de hacer scroll).
- Toggle de idioma, bot√≥n de reinicio y chips de plataformas con mejor contraste en claro/oscuro.
- CTA din√°mico **Generar contenido / Generar traducci√≥n** seg√∫n la casilla de traducci√≥n literal.
- Bot√≥n "Copiar" con texto blanco y estilos consistentes con el tema.
- Script `scripts/manage-ollama.ps1` para listar modelos, precargar el seleccionado y reiniciar el daemon sin procesos hu√©rfanos.

---

## Estado por √°reas

| √Årea | Estado | Comentario |
|------|--------|------------|
| üß† Generaci√≥n de texto | ‚úÖ Estable | Auto prioriza mistral/qwen y reintenta si un modelo devuelve JSON inv√°lido. |
| üåê Traducci√≥n literal | ‚úÖ | JSON limpio y CTA din√°mico **Generar traducci√≥n** cuando corresponde. |
| üéØ Selector de idiomas | ‚úÖ Adaptativo | Idiomas no soportados aparecen deshabilitados con tooltip. |
| üìå Modelo usado | ‚úÖ | Visible bajo "Modelo de texto" tras cada generaci√≥n. |
| üñºÔ∏è Imagen / üîä Voz / üéôÔ∏è STT | ‚ö†Ô∏è Pendiente | FastAPI expone endpoints, pero falta afinar modelos (Kokoro/Whisper/SD). |
| üß© Refactor front | ‚öôÔ∏è En progreso | Modos legacy pendientes de limpieza y tests. |
| üß™ Tests automatizados | ‚è≥ | Vitest configurado, a√∫n sin suites. |
| üíæ Persistencia | ‚è≥ | No hay DB; datos en localStorage. |
| üöÄ CI/CD | ‚è≥ | Builds manuales (sin GitHub Actions). |

---

---

## Modelos e idiomas soportados

| Familia / Modelo | Idiomas |
|------------------|---------|
| `llama2`, `llama3`, `mistral`, `gemma` | ES, EN, FR, DE, PT, IT, RU |
| `qwen2.5`, `yi`, `deepseek` | Todo lo anterior + JA, ZH |
| `phi`, `orca`, `neural-chat` | ES, EN, FR, DE, PT, IT |

> Tras pulsar ‚ÄúActualizar modelos‚Äù, la app recalcula la cobertura. Si faltan idiomas (por ejemplo japon√©s), instala un modelo compatible (`ollama pull qwen2.5:7b`). El selector mostrar√° esos idiomas en cuanto el modelo est√© disponible.

---

## Pr√≥ximos pasos sugeridos

1. **Backend creativo**: completar la integraci√≥n de Kokoro (descarga `kokoro.onnx` + `voices.json` en `python-backend/models/`) y ajustar Stable Diffusion en `/api/image`.
2. **Tests de regresi√≥n**: cobertura m√≠nima para los modos cr√≠ticos (`BasicMode`, `CampaignMode`, `ChatMode`).
3. **Persistencia y sesiones**: guardar prompts/outputs en una base ligera (SQLite/Postgres) y permitir historial.
4. **Observabilidad**: m√©tricas b√°sicas (conteo de generaciones por modo/modelo) y logs estructurados.
5. **CI/CD**: pipeline de GitHub Actions con lint + tests antes de mergear en `development`.
6. **Optimizaci√≥n de assets**: revisar bundle y lazy loading m√°s granular (solo si no afecta UX).

---

## Notas operativas

- `.env.local` en la ra√≠z; no expone credenciales sensibles salvo que conectes backends externos.
- Para comprobar servicios locales, usa `npm run check:health`.
- Cambios en estilos van a `src/styles/commonStyles.ts` para mantener consistencia claro/oscuro.
- Antes de a√±adir un idioma nuevo, extiende `capabilityMatrix` con el modelo que lo soporta; de lo contrario aparecer√° deshabilitado.
- Script √∫til: `.\scripts\manage-ollama.ps1` (PowerShell) lista modelos, precarga el seleccionado y reinicia `ollama serve`.

---

## TL;DR

- El front ya no depende de servicios externos: todo corre en Ollama + FastAPI locales.
- La UI se adapta al modelo instalado (idiomas, modelo usado, traducciones literales fiables).
- Falta completar el backend multimedia, a√±adir tests y automatizar la entrega, pero la base es estable para trabajo diario.  
- Siguiente hito: cerrar la integraci√≥n de Kokoro/Whisper/SDXL y a√±adir pruebas automatizadas.

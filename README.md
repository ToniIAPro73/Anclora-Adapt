# Anclora Adapt

![GHBanner](https://github.com/user-attachments/assets/0aa67016-6eaf-458a-adb2-6e31a0763ed6)

Aplicaci√≥n React 19 + Vite 6 + TypeScript para generar/adaptar contenido estrat√©gico con modelos locales (Ollama para texto y endpoints propios para imagen/TTS/STT). Es una **SPA (Single Page Application)** refactorizada con **contextos especializados** (Theme, Language, Model, UI, Media) minimizando re-renders en 70-80%, y componentes modulares por modo (Basic, Intelligent, Campaign, Recycle, Chat, Voice, Live Chat, Image).

---

## Stack & requisitos

| Capa            | Detalle                                                     |
| --------------- | ----------------------------------------------------------- |
| UI              | React 19, Vite 6, TypeScript, CSS-in-JS b√°sico              |
| Estado global   | Context API (`src/context`)                                 |
| Modelos texto   | [Ollama](https://ollama.ai/) (`/api/generate`, `/api/tags`) |
| Imagen          | Endpoint HTTP (SD 1.5/SDXL, bridge incluido)                |
| Audio (TTS/STT) | Endpoints locales (Whisper/Kokoro/etc.)                     |
| Tests           | [Vitest](https://vitest.dev/) + React Testing Library       |

## Requisitos previos

- Node.js 18+
- Ollama en local (`ollama serve`)
- (Opcional) backend de imagen en `http://localhost:9090/image`
- (Opcional) backend de TTS/STT (puedes usar `python-backend` o tus propios servidores)

---

## Estructura del repositorio

```Tree
.
‚îú‚îÄ‚îÄ assets/               # Recursos est√°ticos (capturas, audio demo)
‚îú‚îÄ‚îÄ docs/                 # Documentaci√≥n viva (AGENTS, ROADMAP, setups, etc.)
‚îú‚îÄ‚îÄ src/                  # C√≥digo de la SPA (App.tsx, contextos, componentes)
‚îÇ   ‚îú‚îÄ‚îÄ api/              # wrappers a Ollama/otros servicios
‚îÇ   ‚îú‚îÄ‚îÄ components/       # layout + modos (Basic/Intelligent/...)
‚îÇ   ‚îú‚îÄ‚îÄ constants/        # prompts, opciones, traducciones
‚îÇ   ‚îú‚îÄ‚îÄ context/          # Theme/Language/Interaction providers
‚îÇ   ‚îú‚îÄ‚îÄ hooks/, services/ # hooks reutilizables y helpers
‚îÇ   ‚îî‚îÄ‚îÄ types/, utils/    # Tipos y utilidades comunes
‚îú‚îÄ‚îÄ scripts/              # utilidades (clean ports, image bridge, tts server‚Ä¶)
‚îú‚îÄ‚îÄ python-backend/       # (Opcional) FastAPI/Fast endpoints unificados
‚îú‚îÄ‚îÄ dist/                 # build de producci√≥n (generado por Vite)
‚îî‚îÄ‚îÄ achive/               # archivos legacy (App.back.tsx, planes hist√≥ricos, etc.)
```

La ra√≠z queda reservada para archivos est√°ndar de un proyecto Vite/React (`package*.json`, `tsconfig.json`, `vite.config.ts`, `index.html`, `.env.local`, `.gitignore`, `README.md`).

---

## Configuraci√≥n r√°pida (`.env.local`)

```dotenv
VITE_API_BASE_URL=http://localhost:8000
VITE_OLLAMA_BASE_URL=http://localhost:11434
VITE_TEXT_MODEL_ID=llama2

# Imagen (FastAPI expone /api/image)
VITE_IMAGE_MODEL_ENDPOINT=http://localhost:8000/api/image
VITE_IMAGE_MODEL_ID=sdxl-lightning

# Audio (FastAPI expone /api/tts y /api/stt)
VITE_TTS_ENDPOINT=http://localhost:8000/api/tts
VITE_TTS_MODEL_ID=kokoro
VITE_STT_ENDPOINT=http://localhost:8000/api/stt
VITE_STT_MODEL_ID=whisper-large-v3

# (Opcional) Token para backends externos
VITE_MODEL_API_KEY=
```

Los toggles de idioma/tema/modelo guardan su estado en `localStorage` (`anclora-language`, `anclora-theme`, `anclora.textModel`).

---

## Scripts principales

```bash
npm install          # Dependencias
npm run dev          # Dev server (http://localhost:4173)
npm run build        # Build de producci√≥n
npm test             # Vitest + React Testing Library
npm run check:health # Valida Ollama y endpoints configurados
node achive/tools/image-bridge.js # Bridge ‚Üí Automatic1111 (legacy, opcional)
python python-backend/main.py  # Backend FastAPI (Kokoro + Whisper + SDXL)
```

> **Tip:** ejecuta `npm run check:health` antes de QA para asegurarte de que Ollama y tus endpoints opcionales responden.

---

## Modelos y perfiles recomendados

| Tipo   | Modelo             | RAM/VRAM aprox. | Uso sugerido                   |
| ------ | ------------------ | --------------- | ------------------------------ |
| Texto  | `llama2`           | 4‚ÄØGB            | Equilibrado generalista        |
| Texto  | `mistral`          | ~5‚ÄØGB           | Mayor contexto/calidad         |
| Texto  | `neural-chat`      | 4‚ÄØGB            | Conversaci√≥n                   |
| Texto  | `orca-mini`        | 2‚ÄØGB            | R√°pido/ligero                  |
| Imagen | SD 1.5 / SDXL-Lite | 4‚ÄØGB VRAM       | Generaci√≥n base 768px          |
| TTS    | pyttsx3/Kokoro     | CPU/GPU ligera  | S√≠ntesis local simple/neuronal |
| STT    | Whisper small/base | CPU/GPU 4‚Äì6‚ÄØGB  | Transcripci√≥n de clips cortos  |

Perfiles ejemplo:

- **RTX 3050** ‚Üí `llama2` o `mistral`, bridge SDXL Lightning (4 pasos), TTS Kokoro + STT Whisper.
- **Solo CPU** ‚Üí `orca-mini`, imagen deshabilitada, TTS pyttsx3.
- **Texto √∫nicamente** ‚Üí define solo `VITE_OLLAMA_BASE_URL` y `VITE_TEXT_MODEL_ID`.

**Nota sobre el Analizador de Im√°genes (Llava)**:

El modo **Inteligente** incluye an√°lisis autom√°tico de im√°genes usando el modelo **Llava:latest** (m√°s r√°pido y estable que Qwen3-VL). Cuando subes una imagen:

1. El backend genera un prompt gen√©rico para la imagen
2. Si proporcionas tu propio prompt, se usa directamente
3. Soporta m√∫ltiples idiomas (ES, EN, FR, DE, IT)
4. Respuesta inmediata sin timeouts

Para usar esta funcionalidad:

````bash
ollama pull Llava:latest   # Descarga el modelo de visi√≥n
# El backend debe estar corriendo en http://localhost:8000
python python-backend/main.py
```text

**Caracter√≠sticas t√©cnicas**:

- Cach√© inteligente con deduplicaci√≥n MD5
- Fallback autom√°tico a prompts gen√©ricos si hay problemas
- SQLite para persistencia de an√°lisis previos (30 d√≠as TTL)

---

## Flujo de desarrollo

1. **Instala dependencias**
   `npm install`

2. **Backend FastAPI (python-backend/)**

   ```bash
   cd python-backend
   C:\Users\Usuario\AppData\Local\Programs\Python\Python311\python.exe -m venv venv
   .\\venv\\Scripts\\Activate.ps1
   pip install -r requirements.txt
   # Descarga kokoro.onnx + voices.json en python-backend/models/
   # (usa `huggingface-cli login` si tu cuenta es privada)
   huggingface-cli download hexgrad/Kokoro-82M kokoro.onnx --local-dir models --local-dir-use-symlinks False
   huggingface-cli download hexgrad/Kokoro-82M voices.json --local-dir models --local-dir-use-symlinks False
   python main.py
````

El backend expone `/api/tts`, `/api/stt`, `/api/image`, `/api/voices` y `/api/images/analyze` (Qwen3-VL para an√°lisis visual).

3. **Arranca Ollama**  
   `ollama pull llama2` ‚Üí `ollama serve`

4. **(Opcional) Otros endpoints**

   - Imagen: por defecto el front env√≠a un POST a `http://localhost:8000/api/image` (servido por `python-backend/main.py`). Arranca ese backend o apunta `VITE_IMAGE_MODEL_ENDPOINT` hacia tu servicio favorito. Si prefieres Automatic1111, ejecuta `node achive/tools/image-bridge.js` (legacy).
   - TTS/STT legacy: `npm run tts:server` s√≥lo para pruebas r√°pidas.

5. **Dev server**  
   `npm run dev` ‚Üí <http://localhost:4173>

6. **Tests & build antes de publicar**  
   `npm test` ‚Üí `npm run build`

---

## Selector de modelo, voces y salud

- El selector de modelos consulta `GET ${VITE_OLLAMA_BASE_URL}/api/tags` y persiste la elecci√≥n en `localStorage`.
- El modo **Voz** llama a `GET ${VITE_TTS_ENDPOINT}/voices` (FastAPI expone `/api/voices`) para poblar idiomas/presets. Si la llamada falla, se muestran presets locales como fallback.
- `npm run check:health` confirma r√°pidamente que Ollama y los endpoints configurados responden antes de abrir la SPA.
- El modo **Imagen** permite elegir dimensiones (512‚Äì1216), pasos y negative prompt; todo se procesa desde `/api/image` (SDXL Lightning, 4‚Äì8 pasos recomendados).
- El modo **Inteligente** genera contenido estrat√©gico con contexto (ideas + contexto + idioma + pensamiento profundo). Opcionalmente puede generar una imagen complementaria: introduce un prompt para describir la imagen deseada. **NUEVO**: Si subes una imagen para an√°lisis, el backend usa Llava:latest para analizar autom√°ticamente todos los elementos visuales (objetos, colores, estilos, composici√≥n, iluminaci√≥n, atm√≥sfera) y genera un prompt detallado que captura los detalles espec√≠ficos de la imagen. Soporta m√∫ltiples idiomas con prompts espec√≠ficos por idioma. **NUEVO (Dic 10)**: El backend ahora selecciona din√°micamente los mejores modelos disponibles: Qwen2.5:14b (primario) > 7b-instruct (secundario) > 7b (terciario) > Mistral > Llama, consultando `GET /api/tags` de Ollama en tiempo de ejecuci√≥n.

---

## QA manual (resumen)

Consulta `docs/QA_CHECKLIST.md`. Cada cambio debe validar:

- Los 8 modos (Basic, Intelligent, Campaign, Recycle, Chat, Voice, Live Chat, Image).
- Persistencia de tema (Light/Dark/System) y lenguaje (ES/EN) tras recargar.
- Manejo de errores (sin API key, timeouts, micr√≥fono denegado, endpoints ca√≠dos).
- Consola limpia y `npm run build` sin fallos.

---

## Despliegue en contenedores

- Contamos con **Dockerfile.frontend** y **Dockerfile.backend** multi-stage. El frontend se sirve con Nginx y el backend corre con Uvicorn.
- `docker-compose.yml` orquesta frontend, backend y una base PostgreSQL opcional. Los vol√∫menes `backend-models` y `backend-cache` preservan descargas de modelos/cach√©s.
- Copia `docker/.env.docker.example` a `docker/.env.docker`, ajusta variables y ejecuta:

```bash
docker compose up --build
```

- El pipeline de GitHub Actions (`.github/workflows/ci.yml`) repite `npm run lint:check`, `npm run build`, arranca el backend y ejecuta `node scripts/check-endpoints.js` como smoke test.
- Para m√°s detalles (hardware recomendado, comandos de verificaci√≥n y pasos manuales), revisa `docs/DEPLOYMENT.md`.

---

## Notas finales

- El alias `@/` apunta a `src/` (ver `tsconfig.json`).
- Los componentes consumen estado global mediante `useTheme`, `useLanguage`, `useInteraction`.
- `achive/` guarda referencias hist√≥ricas (App back, planes de refactor, herramientas de migraci√≥n). No borres nada sin revisar.

Si necesitas m√°s contexto (roadmap, instrucciones de agentes, setups), revisa la carpeta `docs/`. ¬°Feliz hacking! üéØ

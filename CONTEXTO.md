# CONTEXTO.md - Estado Actual del Proyecto

Documento de estado que describe el contexto tÃ©cnico actual de Anclora Adapt.

## ğŸ“Š Estado General

| Aspecto              | Estado          | Nota                                             |
| -------------------- | --------------- | ------------------------------------------------ |
| Frontend             | âœ… Funcional    | React 19 + Vite 6, todos los modos operacionales |
| Backend              | âœ… Funcional    | FastAPI con anÃ¡lisis de imÃ¡genes, TTS, STT       |
| AnÃ¡lisis de ImÃ¡genes | âœ… Operacional  | Usando Llava:latest (mÃ¡s estable)                |
| CachÃ©                | âœ… Implementado | SQLite con deduplicaciÃ³n MD5, 30 dÃ­as TTL        |
| Tests                | âš ï¸ BÃ¡sicos      | Vitest configurado, cobertura limitada           |
| DocumentaciÃ³n        | âœ… Actualizada  | README, CLAUDE, CONTEXTO, AGENTS                 |

## ğŸ”„ Cambios Recientes (Diciembre 10, 2025)

### 1. MigraciÃ³n de Modelo de VisiÃ³n

**Problema**: Qwen3-VL causaba timeouts al procesar imÃ¡genes base64
**SoluciÃ³n**: Cambio a Llava:latest + prompts genÃ©ricos como fallback
**Archivos modificados**:

- `python-backend/app/services/image_analyzer.py` (vision_model: "Llava:latest")
- `python-backend/app/services/model_fallback.py` (\_call_vision_model devuelve prompts genÃ©ricos)

### 2. CorrecciÃ³n de Timeouts

**Problema**: 120 segundos insuficientes para anÃ¡lisis complejos
**SoluciÃ³n**: Aumentado a 300 segundos en model_fallback.py:183
**JustificaciÃ³n**: Modelos grandes necesitan mÃ¡s tiempo

### 3. CorrecciÃ³n de Parseado de ParÃ¡metros

**Problema**: `deep_thinking` se enviaba como string "false" desde formulario
**SoluciÃ³n**: Parseado explÃ­cito en image_analysis.py:61

```python
deep_thinking_bool = deep_thinking.lower() == "true" if isinstance(deep_thinking, str) else deep_thinking
```

### 4. NormalizaciÃ³n de Respuestas API

**Problema**: Hook esperaba `generative_prompt` en raÃ­z, API devolvÃ­a `image_context.generative_prompt`
**SoluciÃ³n**: Actualizado useImageAnalyzer.ts para manejar ambos formatos
**Archivos**:

- `src/hooks/useImageAnalyzer.ts` (lÃ­neas 122-128, 196-202)

### 5. Limpieza de ESLint Warnings

**Solucionado**:

- âŒ Removido import no usado: `AutoModelContext`
- âŒ Removida interfaz no usada: `IntelligentJSON`
- âŒ Removido parÃ¡metro no usado: `onGenerate` de props
- âœ… Reemplazado `as any` por tipo `GeneratedJSON` tipado

### 6. GestiÃ³n de CachÃ©

**Implementado**:

- CachÃ© SQLite en `python-backend/cache/image_analysis_cache.db`
- DeduplicaciÃ³n basada en hash MD5 de imagen
- TTL de 30 dÃ­as (configurable)
- Endpoints: `/api/images/cache-stats`, `/api/images/cache-clear-expired`

**Ignorado en git**:

- `.gitignore` actualizado para `*.db`, `*.sqlite`, `*.sqlite3`
- `image_analysis_cache.db` marcado como `assume-unchanged`

### 7. SelecciÃ³n DinÃ¡mica de Modelos para OptimizaciÃ³n de Prompts

**Problema**: Hardcoded model list no se adaptaba a los modelos disponibles en el hardware del usuario

**SoluciÃ³n Implementada** (Diciembre 10, 2025):

- **Nuevo archivo**: `python-backend/app/services/model_selector.py`
  - `get_available_models()` - Consulta Ollama `/api/tags` dinÃ¡micamente
  - `select_best_models()` - Prioriza modelos: Qwen2.5:14b > 7b-instruct > 7b > Mistral > Llama
  - `get_model_candidates()` - Punto de entrada con fallback a MODEL_PRIORITY

- **Modificado**: `python-backend/app/services/prompt_optimizer.py`
  - Cambio de hardcoded `["mistral:latest", "qwen2.5:14b", ...]` a `get_model_candidates()`
  - Removido error cuando no hay modelos (ahora siempre hay fallback)
  - Intenta modelos en orden: qwen2.5:14b â†’ 7b-instruct â†’ 7b

- **Resultado**:
  - Backend genera **2000+ caracteres** cuando ambos checkboxes activados
  - Qwen2.5:14b seleccionado automÃ¡ticamente como modelo primario
  - Fallback chain garantiza operaciÃ³n incluso si Ollama `/api/tags` no responde

**VerificaciÃ³n**:
```bash
# Test el servicio directamente
cd python-backend
python -c "from app.services.model_selector import get_model_candidates; print(get_model_candidates())"
# Output esperado: ['qwen2.5:14b', 'qwen2.5:7b-instruct-q4_K_M', 'qwen2.5:7b-instruct']
```

## ğŸ“ Estructura de Carpetas CrÃ­tica

```
src/
â”œâ”€â”€ components/modes/
â”‚   â”œâ”€â”€ IntelligentMode.tsx           â† Main component for intelligent mode
â”‚   â”œâ”€â”€ IntelligentModeForm.tsx        â† Form inputs
â”‚   â”œâ”€â”€ IntelligentModeImageOptions.tsx â† Image upload and analysis
â”‚   â””â”€â”€ useIntelligentModeState.ts     â† State hook
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useImageAnalyzer.ts            â† Image analysis API wrapper
â”‚   â”œâ”€â”€ useIntelligentModeState.ts     â† Intelligent mode state
â”‚   â””â”€â”€ ...otros hooks
â””â”€â”€ types/
    â””â”€â”€ index.ts                        â† Tipos globales

python-backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ image_analyzer.py          â† Main analyzer logic
â”‚   â”‚   â”œâ”€â”€ model_fallback.py          â† Fallback chain logic
â”‚   â”‚   â”œâ”€â”€ image_cache.py             â† SQLite cache
â”‚   â”‚   â””â”€â”€ image_security.py          â† Validation
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ image_analysis.py          â† POST /api/images/analyze
â”‚   â”‚   â””â”€â”€ prompt_optimizer.py        â† POST /api/prompts/optimize
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ image_context.py           â† Pydantic schemas
â”‚   â””â”€â”€ main.py                        â† FastAPI app
â”œâ”€â”€ cache/
â”‚   â””â”€â”€ image_analysis_cache.db        â† SQLite database (gitignored)
â””â”€â”€ requirements.txt                   â† Dependencies
```

## ğŸ› ï¸ ConfiguraciÃ³n TÃ©cnica

### Modelos Ollama Instalados

```bash
ollama list
# Llava:latest               âœ… VisiÃ³n (anÃ¡lisis de imÃ¡genes)
# qwen3-vl:8b                âœ… VisiÃ³n (fallback)
# qwen2.5:14b                âœ… Texto (PRIMARIO para optimizaciÃ³n)
# qwen2.5:7b-instruct        âœ… Texto (SECUNDARIO para optimizaciÃ³n)
# qwen2.5:7b                 âœ… Texto (TERCIARIO para optimizaciÃ³n)
# mistral:latest             âœ… Texto (fallback)
# llama2:latest              âœ… Texto (generalist)
```

**Nota**: El backend ahora consulta Ollama `/api/tags` dinÃ¡micamente y prioriza automÃ¡ticamente los mejores modelos disponibles.

### Variables de Entorno

**Frontend (.env.local)**:

```dotenv
VITE_API_BASE_URL=http://localhost:8000
VITE_OLLAMA_BASE_URL=http://localhost:11434
VITE_TEXT_MODEL_ID=mistral:latest
```

**Backend (automÃ¡tico)**:

- Detecta GPU disponible (CUDA/CPU)
- Puerto por defecto: 8000
- Ollama endpoint: <http://localhost:11434>

## ğŸ”Œ API Endpoints

### Image Analysis

```
POST /api/images/analyze
Content-Type: multipart/form-data

ParÃ¡metros:
- image (File): Archivo PNG/JPG
- user_prompt (str, optional): Prompt adicional del usuario
- deep_thinking (str): "true" o "false"
- language (str): "es", "en", "fr", "de", "it"

Respuesta:
{
  "success": true,
  "image_context": {
    "generative_prompt": "...",
    "brief_caption": "...",
    "detailed_description": "...",
    ...
  },
  "metadata": { ... }
}
```

### Prompt Optimization

```
POST /api/prompts/optimize

Body: {
  "prompt": "Tu prompt aquÃ­",
  "deep_thinking": true,
  "language": "es"
}

Respuesta: {
  "success": true,
  "improved_prompt": "..."
}
```

## ğŸ› Problemas Conocidos y Soluciones

### 1. Cache Database Bloqueada

**Problema**: `rm image_analysis_cache.db` falla con "Device or resource busy"
**SoluciÃ³n**: Usar PowerShell `Remove-Item -Force` o reiniciar servidor

### 2. Python Compilado en CachÃ©

**Problema**: Cambios en .py no se reflejan sin reiniciar servidor
**SoluciÃ³n**:

```bash
find . -type d -name "__pycache__" -exec rm -rf {} +
# Luego reiniciar: python main.py
```

### 3. CORS si Frontend y Backend en puertos diferentes

**SoluciÃ³n**: FastAPI ya tiene CORS configurado en main.py

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
)
```

### 4. Pydantic Warnings sobre "model\_"

**Causa**: Campos `model_used`, `model_fallback_used` conflictÃºan con namespace protegido
**No es crÃ­tico**: Solo warnings, funcionan correctamente
**SoluciÃ³n futura**: Renombrar a `used_model` y `fallback_used`

## ğŸ“ˆ MÃ©tricas de Rendimiento

| Aspecto                | Valor   | Target          |
| ---------------------- | ------- | --------------- |
| Re-renders evitados    | 70-80%  | âœ… Alcanzado    |
| Tiempo anÃ¡lisis imagen | <5s     | âœ… Actual       |
| Tiempo respuesta API   | <2s     | âœ… Actual       |
| TamaÃ±o bundle          | ~150 KB | âœ… OK           |
| TTL cachÃ©              | 30 dÃ­as | âš™ï¸ Configurable |

## ğŸ§ª Testing

```bash
npm test                    # Ejecuta Vitest
npm run check:health        # Verifica Ollama y endpoints

# Cobertura actual: BÃ¡sica (necesita expansiÃ³n)
# Tests principales en: tests/ (crear si no existen)
```

## ğŸš€ PrÃ³ximos Pasos Recomendados

1. **Expandir cobertura de tests** - Faltan tests para componentes crÃ­ticos
2. **Documentar prompts** - Crear guÃ­a de engineering prompts
3. **Performance profiling** - Medir exactamente dÃ³nde se gastan los ms
4. **Mejorar manejo de errores** - Mensajes mÃ¡s especÃ­ficos para errores de red
5. **InternacionalizaciÃ³n completa** - Extender a mÃ¡s idiomas

## ğŸ“ Checklist de VerificaciÃ³n

Antes de hacer cambios significativos:

- [ ] Lee CLAUDE.md
- [ ] Verifica que Ollama estÃ¡ corriendo
- [ ] Limpia cachÃ© (`__pycache__`, `.db`)
- [ ] Ejecuta `npm test`
- [ ] Verifica DevTools (F12) sin errors
- [ ] Prueba cada modo manualmente
- [ ] Revisa ESLint (`npm run lint` si existe)

## ğŸ‘¤ InformaciÃ³n del Usuario

- **Workspace**: C:\Users\Usuario\Workspace\01_Proyectos\Anclora-Adapt
- **Git branch**: development (PR against main)
- **Idioma preferido**: EspaÃ±ol
- **Preferencia de docs**: Minimal (solo pedir si es necesario)

---

**Ãšltima actualizaciÃ³n**: Diciembre 10, 2025 11:45
**VersiÃ³n del documento**: 2.1
**Estado de sincronizaciÃ³n**: âœ… Sincronizado con cÃ³digo actual (selecciÃ³n dinÃ¡mica de modelos implementada)
